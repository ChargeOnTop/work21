# Agent Estimator Service - Документация для AI

## Назначение

Микросервис на Spring Boot для взаимодействия с локальной LLM (Ollama) для оценки трудозатрат и стоимости IT-проектов. Принимает ТЗ, отправляет запросы к LLM через Ollama API, парсит JSON-ответы и возвращает структурированные оценки.

## Технологический стек

- **Spring Boot 3.2.0** + **Java 17**
- **Spring Cloud OpenFeign** — HTTP клиент для Ollama API
- **Lombok**, **Jackson**, **Jakarta Validation**

## Архитектура

**Поток данных:**
1. Клиент → REST API (`/api/v1/llm/chat` или `/api/v1/llm/ask`)
2. `LlmController` → валидация запроса
3. `LlmService` → обогащение запроса системным промптом и параметрами стабильности
4. `OllamaClient` (Feign) → HTTP запрос к Ollama API (`http://localhost:11434/api/chat`)
5. Ollama → генерация ответа LLM
6. `LlmService` → парсинг JSON ответа, извлечение структурированной оценки
7. `LlmController` → возврат ответа клиенту

## API Endpoints

- **POST `/api/v1/llm/chat`** — полный запрос с историей сообщений
- **POST `/api/v1/llm/ask`** — упрощённый запрос с одним промптом

## Логика работы

### Обогащение запросов

Сервис автоматически:
1. Добавляет системный промпт из `application.yml` (если не задан в запросе)
2. Применяет параметры стабильности по умолчанию (temperature: 0.3, topP: 0.9, topK: 40, repeat-penalty: 1.1, num-predict: 512)
3. Объединяет системные промпты из запроса и конфигурации

### Парсинг JSON ответов

Сервис использует регулярные выражения для поиска JSON в ответе модели, даже если он окружён дополнительным текстом:
```java
Pattern JSON_PATTERN = Pattern.compile("\\{.*\"price\".*\"data\".*\\}", Pattern.DOTALL);
```

Если JSON не найден или невалиден, возвращается полный текстовый ответ без парсинга.

### Системный промпт

Автоматически добавляется ко всем запросам. Настраивает LLM как эксперта по оценке трудозатрат:
- Инструкции по оценке (оптимистичная и пессимистичная)
- Правила разбиения ТЗ на задачи
- Формат ответа (JSON с полями `price` и `data`)

## Формат ответа

Сервис ожидает от LLM JSON:
```json
{
  "price": 16000,
  "data": "Оптимистичная: 8 часов, Пессимистичная: 16 часов\n\nЗадачи:\n1. Анализ требований (1ч)\n2. Разработка (4-8ч)..."
}
```

Ответ сервиса включает:
- `price` — бюджет в рублях (извлечён из JSON)
- `estimation` — полный объект `EstimationResponse`
- `message.content` или `response` — исходный ответ от LLM

## Интеграция с Ollama

- **Feign Client** (`OllamaClient`) для HTTP запросов
- **Таймауты**: connect 10s, read 120s
- **URL**: `http://localhost:11434` (настраивается через `OLLAMA_API_URL`)

## Обработка ошибок

- **400** — валидация запроса
- **503** — недоступность Ollama API (FeignException)
- **500** — внутренние ошибки

Все ошибки логируются и возвращаются в структурированном формате.

